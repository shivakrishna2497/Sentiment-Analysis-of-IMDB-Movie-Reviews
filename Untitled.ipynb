{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDb Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB movie reviews dataset is a set of 50,000 reviews, half of which are positive and the other half negative. \n",
    "\n",
    "\n",
    "The dataset is available online and can be either directly downloaded from Stanford’s website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data folder called aclImdb. From there, we can use the load_train_test_imdb_data function to load the training/test datasets from IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_imdb_data(data_dir):\n",
    "    \"\"\"Loads the IMDB train/test datasets from a folder path.\n",
    "    Input:\n",
    "    data_dir: path to the \"aclImdb\" folder.\n",
    "    \n",
    "    Returns:\n",
    "    train/test datasets as pandas dataframes.\n",
    "    \"\"\"\n",
    "\n",
    "    data = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        data[split] = []\n",
    "        for sentiment in [\"neg\", \"pos\"]:\n",
    "            score = 1 if sentiment == \"pos\" else 0\n",
    "\n",
    "            path = os.path.join(data_dir, split, sentiment)\n",
    "            file_names = os.listdir(path)\n",
    "            for f_name in file_names:\n",
    "                with open(os.path.join(path, f_name), \"r\") as f:\n",
    "                    review = f.read()\n",
    "                    data[split].append([review, score])\n",
    "\n",
    "    np.random.shuffle(data[\"train\"])        \n",
    "    data[\"train\"] = pd.DataFrame(data[\"train\"],\n",
    "                                 columns=['text', 'sentiment'])\n",
    "    print(data[\"train\"])\n",
    "    np.random.shuffle(data[\"test\"])\n",
    "    data[\"test\"] = pd.DataFrame(data[\"test\"],\n",
    "                                columns=['text', 'sentiment'])\n",
    "    print(data[\"test\"])\n",
    "    return data[\"train\"], data[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  sentiment\n",
      "0      After the initial shock of realizing the guts ...          1\n",
      "1      'The Shop Around the Corner (1940)' is a pleas...          1\n",
      "2      How this movie escaped the wrath of MST3K I'll...          0\n",
      "3      A romanticised and thoroughly false vision of ...          0\n",
      "4      I, like many die-hard Trekkers (or Trekkies, i...          1\n",
      "5      Nacho Vigalondo is very famous in Spain. He is...          0\n",
      "6      In the areas where they overlap this fine movi...          1\n",
      "7      Okay... it seems like so far, only the Barman ...          0\n",
      "8      This movie was one of the best movies that I h...          1\n",
      "9      I love this movie. It's wacky, funny, violent,...          1\n",
      "10     I agree with one of the other comment writers ...          0\n",
      "11     I was lucky enough to see this at a pre-screen...          1\n",
      "12     If this film strikes you (as it did us and, ap...          1\n",
      "13     This is going to be the most useless comment I...          0\n",
      "14     An enjoyable Batman animated film. Not on par ...          1\n",
      "15     I love MIDNIGHT COWBOY and have it in my video...          1\n",
      "16     It was a decent movie, I actually kind of enjo...          0\n",
      "17     Another Pokemon movie has hit the theaters, an...          1\n",
      "18     This was thought to be the flagship work of th...          0\n",
      "19     The short that starts this film is the true fo...          1\n",
      "20     Yes i'll say before i start commenting, this m...          1\n",
      "21     I liked this TV show because it was it's own t...          1\n",
      "22     I saw the The Bourne Ultimatum last summer wit...          1\n",
      "23     The supernatural, vengeful police officer is b...          0\n",
      "24     The Sentinel represents everything about the s...          0\n",
      "25     I got a good laugh reading all the idiotic com...          1\n",
      "26     The plot is straightforward an old man living ...          0\n",
      "27     This movie surprised me in a good way. From th...          1\n",
      "28     Much to your presumable happiness fair readers...          1\n",
      "29     I just watched this movie and I've gotta say t...          0\n",
      "...                                                  ...        ...\n",
      "24970  This was a pretty decent movie. This movie is ...          1\n",
      "24971  Doc Savage: The Man of Bronze is a horrible mo...          0\n",
      "24972  This film is shoddily-made, unoriginal garbage...          0\n",
      "24973  i bought this DVD because it has kari in it an...          0\n",
      "24974  Although I like Kurt Vonnegut, I'm not particu...          1\n",
      "24975  It was just a terrible movie. No one should wa...          0\n",
      "24976  David Burton(Richard Chamberlain, quite good)i...          1\n",
      "24977  I'm afraid this one is pretty dreadful, despit...          0\n",
      "24978  this movie had a lot of blood in it when the s...          0\n",
      "24979  As a popular sport, surfing was liked by many ...          1\n",
      "24980  While the acting and directing could be argued...          0\n",
      "24981  Bellocchio refers to this as a mainly politica...          1\n",
      "24982  I saw The Big Bad Swim at the 2006 Temecula fi...          1\n",
      "24983  Lets make a movie about a talk show that alrea...          0\n",
      "24984  What?!?? Why are people saying this is \"mind b...          0\n",
      "24985  I found this to be the most enjoyable Muppets ...          1\n",
      "24986  What a strangely wonderful, if sometimes sligh...          1\n",
      "24987  The film looks super on paper. A romantic come...          0\n",
      "24988  Thunderbirds (2004) <br /><br />Director: Jona...          0\n",
      "24989  Well, I thoroughly enjoyed this movie. It was ...          1\n",
      "24990  Okay, I rented this movie because of the direc...          0\n",
      "24991  I've always enjoyed films that depict life as ...          1\n",
      "24992  Somebody could probably make a great documenta...          0\n",
      "24993  This movie is light, funny, and beautifully fi...          1\n",
      "24994  If you haven't figured out what is going to ha...          0\n",
      "24995  This very strange movie is unlike anything mad...          1\n",
      "24996  When I sat down to watch 'Largo Winch' I expec...          1\n",
      "24997  Whatever happened to British TV drama? From Jo...          0\n",
      "24998  When robot hordes start attacking major cities...          0\n",
      "24999  First off, I had my doubts just looking at the...          0\n",
      "\n",
      "[25000 rows x 2 columns]\n",
      "                                                    text  sentiment\n",
      "0      I rarely write reviews but this film simply de...          1\n",
      "1      A fine line up of actors and a seemingly nice ...          0\n",
      "2      \"The Violent Men\" marked the finest collaborat...          1\n",
      "3      This movie is beyond Horrible AVOID AT ALL COS...          0\n",
      "4      This is not a story. It's a bunch of psychic n...          1\n",
      "5      Stan Laurel and Oliver Hardy are the most famo...          0\n",
      "6      This miniseries is a reasonable sequel to the ...          1\n",
      "7      I was more entertained by watching my wife alm...          0\n",
      "8      I was suckered in by the big names. Rob Lowe, ...          0\n",
      "9      I am a long time fan of Luc Besson's work, and...          1\n",
      "10     We loved this movie because it was so entertai...          1\n",
      "11     There's not a dull moment in Francois Ozon's \"...          1\n",
      "12     Prior to Airport 79' these movies were rather ...          0\n",
      "13     I saw this movie only because Sophie Marceau. ...          0\n",
      "14     Maybe in its day this movie was special. But f...          0\n",
      "15     This movie is well made, it is beautiful and w...          1\n",
      "16     Watching this movie really surprised me. I hav...          0\n",
      "17     Best thing I can say about this porno-horror f...          0\n",
      "18     It must say something about the state of our n...          0\n",
      "19     Okay, granted, I am a fan of low-budget horror...          0\n",
      "20     When a bomber, a patricide, a pornographer, an...          1\n",
      "21     I was wondering if there was a place or a link...          0\n",
      "22     Despite the fact that there were aspects of th...          1\n",
      "23     I don't normally write reviews, in fact, I nev...          0\n",
      "24     One would make you believe that this game is a...          0\n",
      "25     I discovered \"The Patriot\" in a DVD-store and ...          0\n",
      "26     There are spoilers but trust me, I'm doing you...          0\n",
      "27     If this is classed as 'real life' of London, t...          0\n",
      "28     Widely known as \"Don't Look in the Basement\" -...          1\n",
      "29     I can't remember the last time a movie was so ...          0\n",
      "...                                                  ...        ...\n",
      "24970  I blame \"Birth of a Nation\" myself - for comme...          1\n",
      "24971  The 1970s are often regarded as a golden age o...          1\n",
      "24972  At least one kind. Very human and moving. Not ...          1\n",
      "24973  I managed to avoid reading Hemingway in colleg...          0\n",
      "24974  Rabbit Seasoning is one of three cartoons that...          1\n",
      "24975  If you first saw this movie with Mary of the F...          0\n",
      "24976  While many people found this film simply too s...          1\n",
      "24977  This film launched my theory about films based...          1\n",
      "24978  A really funny British comedy from the mid 195...          1\n",
      "24979  I have to agree with what many of the other re...          0\n",
      "24980  It was like someone was trying to make a scary...          0\n",
      "24981  Heart of Darkness Movie Review Joseph Conrad's...          0\n",
      "24982  (Spoilers) \"Cash Crop\" goes something like thi...          0\n",
      "24983  The filming is cheesy. Some of the actors over...          1\n",
      "24984  This certainly is one of the best western movi...          1\n",
      "24985  Remember that friend in college who always ins...          1\n",
      "24986  The first twenty-five minutes stand out as pos...          0\n",
      "24987  I watched this movie the other night, and I ha...          1\n",
      "24988  There have been many movies, on living the Ame...          0\n",
      "24989  This is an extraordinary topical thriller. Fon...          1\n",
      "24990  You can tell that this is the first offering b...          1\n",
      "24991  The plot: Four people are caught in an elevato...          0\n",
      "24992  I saw Conrack on a night I couldn't sleep and ...          1\n",
      "24993  This crock of doodoo won a award? They must ha...          0\n",
      "24994  First, I loved the documentary. It represents ...          1\n",
      "24995  A throwback to the \"old fashioned\" Westerns of...          1\n",
      "24996  I loved this film. A must see for any Rod Stei...          1\n",
      "24997  This movie is the very worst that I have ever ...          0\n",
      "24998  This movie was a low point for both Jason Roba...          0\n",
      "24999  (Some Spoilers) Sweeping into New York City on...          1\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = load_train_test_imdb_data(\n",
    "    data_dir=\"aclImdb/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'not', 'a', 'sentence']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"<div>This is not a sentence.<\\div>\").split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train a sentiment analysis classifier. One thing to keep in mind is that the feature vectors that result from BOW are usually very large (80,000-dimensional vectors in this case). So we need to use simple algorithms that are efficient on a large number of features (e.g., Naive Bayes, linear SVM, or logistic regression). Let’s train a linear SVM classifier for example.\n",
    "\n",
    "Because the IMDB dataset is balanced, we can evaluate our model using the accuracy score (i.e., the proportion of samples that were correctly classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features  (0, 41711)\t1\n",
      "  (0, 46720)\t1\n",
      "  (0, 47352)\t1\n",
      "  (0, 51636)\t1\n",
      "  (0, 41463)\t1\n",
      "  (0, 16130)\t1\n",
      "  (0, 21603)\t1\n",
      "  (0, 47382)\t1\n",
      "  (0, 78531)\t1\n",
      "  (0, 41732)\t1\n",
      "  (0, 62762)\t1\n",
      "  (0, 23353)\t1\n",
      "  (0, 45974)\t1\n",
      "  (0, 8270)\t1\n",
      "  (0, 1496)\t1\n",
      "  (0, 63423)\t1\n",
      "  (0, 73151)\t1\n",
      "  (0, 19890)\t1\n",
      "  (0, 2382)\t1\n",
      "  (0, 38186)\t1\n",
      "  (0, 23637)\t1\n",
      "  (0, 32137)\t1\n",
      "  (0, 24419)\t1\n",
      "  (0, 63438)\t1\n",
      "  (0, 41757)\t1\n",
      "  :\t:\n",
      "  (24999, 77034)\t1\n",
      "  (24999, 65051)\t1\n",
      "  (24999, 46110)\t1\n",
      "  (24999, 55179)\t1\n",
      "  (24999, 57644)\t1\n",
      "  (24999, 9260)\t1\n",
      "  (24999, 25049)\t1\n",
      "  (24999, 21940)\t1\n",
      "  (24999, 71200)\t1\n",
      "  (24999, 67836)\t1\n",
      "  (24999, 38813)\t1\n",
      "  (24999, 71642)\t1\n",
      "  (24999, 27740)\t1\n",
      "  (24999, 10313)\t1\n",
      "  (24999, 41787)\t1\n",
      "  (24999, 40)\t1\n",
      "  (24999, 19530)\t1\n",
      "  (24999, 42150)\t1\n",
      "  (24999, 7551)\t1\n",
      "  (24999, 77214)\t1\n",
      "  (24999, 61811)\t1\n",
      "  (24999, 71195)\t1\n",
      "  (24999, 41463)\t2\n",
      "  (24999, 47382)\t4\n",
      "  (24999, 38186)\t2\n",
      "Testing Features  (0, 5188)\t1\n",
      "  (0, 7026)\t1\n",
      "  (0, 13377)\t1\n",
      "  (0, 14462)\t1\n",
      "  (0, 14486)\t1\n",
      "  (0, 14666)\t1\n",
      "  (0, 14760)\t1\n",
      "  (0, 15424)\t1\n",
      "  (0, 18582)\t1\n",
      "  (0, 21382)\t1\n",
      "  (0, 21940)\t1\n",
      "  (0, 23192)\t1\n",
      "  (0, 26084)\t3\n",
      "  (0, 26102)\t1\n",
      "  (0, 28059)\t1\n",
      "  (0, 28136)\t1\n",
      "  (0, 28813)\t1\n",
      "  (0, 29071)\t1\n",
      "  (0, 29872)\t1\n",
      "  (0, 33728)\t1\n",
      "  (0, 34527)\t1\n",
      "  (0, 36900)\t1\n",
      "  (0, 38996)\t1\n",
      "  (0, 40540)\t1\n",
      "  (0, 42265)\t1\n",
      "  :\t:\n",
      "  (24999, 69840)\t1\n",
      "  (24999, 70949)\t1\n",
      "  (24999, 71516)\t1\n",
      "  (24999, 71621)\t2\n",
      "  (24999, 71664)\t1\n",
      "  (24999, 72283)\t1\n",
      "  (24999, 72348)\t1\n",
      "  (24999, 73151)\t1\n",
      "  (24999, 73432)\t1\n",
      "  (24999, 73612)\t2\n",
      "  (24999, 74098)\t1\n",
      "  (24999, 74668)\t1\n",
      "  (24999, 74959)\t1\n",
      "  (24999, 75045)\t2\n",
      "  (24999, 75886)\t1\n",
      "  (24999, 76883)\t1\n",
      "  (24999, 77214)\t1\n",
      "  (24999, 77842)\t1\n",
      "  (24999, 77953)\t1\n",
      "  (24999, 78124)\t1\n",
      "  (24999, 78556)\t1\n",
      "  (24999, 78704)\t1\n",
      "  (24999, 78738)\t1\n",
      "  (24999, 78757)\t1\n",
      "  (24999, 79402)\t4\n",
      "[1 0 1 ... 0 0 1]\n",
      "Accuracy on the IMDB dataset: 83.67\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Transform each text into a vector of word counts\n",
    "vectorizer = CountVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=clean_text\n",
    "                             )\n",
    "training_features = vectorizer.fit_transform(train_data[\"text\"]) \n",
    "print('Training Features' + str(training_features))\n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "print('Testing Features' + str(test_features))\n",
    "# Training\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "print(y_pred)\n",
    "# Evaluation\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, following some very basic steps and using a simple linear model, we were able to reach as high as an 83.67% accuracy on the IMDB dataset. To realize how good this is, a recent state-of-the-art model can get around 95% accuracy. So this isn’t bad at all, but there is still some room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the Current Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting aside anything fine-tuning related, there are some changes we can make to immediately improve the current model.\n",
    "\n",
    "The first thing we can do is improve the vectorization step. In fact, there are some biases attached with only looking at how many times a word occurs in a text. In particular, the longer the text, the higher its features (word counts) will be.\n",
    "\n",
    "To fix this issue, we can use Term Frequency (TF) instead of word counts and divide the number of occurrences by the sequence length. We can also downscale these frequencies so that words that occur all the time (e.g., topic-related or stop words) have lower values. This downscaling factor is called Inverse Document Frequency (IDF) and is equal to the logarithm of the inverse word document frequency.\n",
    "\n",
    "Put together, these new features are are called TF-IDF features.\n",
    "\n",
    "In practice, we can train a new Linear SVM on TF-IDF features simply by replacing the CountVectorizer with a TfIdfVectorizer. This results in an accuracy of 86.64%, which is a 2% improvement over using BOW features.\n",
    "\n",
    "The second thing we can do to further improve our model is to provide it with more context. In fact, considering every word independently can lead to some errors. For instance, if the word good occurs in a text, we will naturally tend to say that this text is positive, even if the actual expression that occurs is actually not good. These mistakes can be easily avoided with the introduction of N-grams.\n",
    "\n",
    "An N-gram is a set of N successive words (e.g., very good [ 2-gram] and not good at all [4-gram]). Using N-grams, we produce richer word sequences.\n",
    "\n",
    "Including N-grams in our TF-IDF vectorizer is as simple as providing an additional parameter ngram_range=(1, N). Generally speaking, the use of bi-grams improves performance, as we provide more context to the model, while higher-order N-grams have less obvious effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features  (0, 802371)\t0.09025965705623717\n",
      "  (0, 1421345)\t0.08149705438412425\n",
      "  (0, 1277615)\t0.10097546672468202\n",
      "  (0, 698733)\t0.09595322966817982\n",
      "  (0, 1058946)\t0.06578975509830308\n",
      "  (0, 186034)\t0.209721397373683\n",
      "  (0, 579942)\t0.024618515726830645\n",
      "  (0, 929634)\t0.07770562225111864\n",
      "  (0, 1413873)\t0.10724381354558746\n",
      "  (0, 525423)\t0.08949794410676692\n",
      "  (0, 720082)\t0.0529985759492894\n",
      "  (0, 505809)\t0.0852939452746549\n",
      "  (0, 847334)\t0.028796607126835218\n",
      "  (0, 52700)\t0.08001956632123572\n",
      "  (0, 427090)\t0.0675906058364338\n",
      "  (0, 1640547)\t0.05470942639432049\n",
      "  (0, 1413588)\t0.09434697876495375\n",
      "  (0, 23135)\t0.10846173798683674\n",
      "  (0, 165800)\t0.10066626350827777\n",
      "  (0, 1020574)\t0.05614456052696763\n",
      "  (0, 498267)\t0.05778390344001641\n",
      "  (0, 1389937)\t0.05534900655597173\n",
      "  (0, 929121)\t0.09156229209556034\n",
      "  (0, 1758559)\t0.06566198870605877\n",
      "  (0, 1047782)\t0.02310697929984962\n",
      "  :\t:\n",
      "  (24999, 1507994)\t0.11016228441673938\n",
      "  (24999, 1713780)\t0.13518656645149235\n",
      "  (24999, 1052839)\t0.13518656645149235\n",
      "  (24999, 1050360)\t0.13083122914224118\n",
      "  (24999, 1610409)\t0.11378408115015805\n",
      "  (24999, 1050587)\t0.14746357278624772\n",
      "  (24999, 1681797)\t0.1389913190261541\n",
      "  (24999, 917047)\t0.14746357278624772\n",
      "  (24999, 944364)\t0.14132506961887004\n",
      "  (24999, 1272631)\t0.15181891009549886\n",
      "  (24999, 1141015)\t0.15795741326287652\n",
      "  (24999, 450734)\t0.15795741326287652\n",
      "  (24999, 182662)\t0.15795741326287652\n",
      "  (24999, 1364350)\t0.15795741326287652\n",
      "  (24999, 204822)\t0.15795741326287652\n",
      "  (24999, 1571834)\t0.15795741326287652\n",
      "  (24999, 643801)\t0.15795741326287652\n",
      "  (24999, 1026560)\t0.15795741326287652\n",
      "  (24999, 2651)\t0.15795741326287652\n",
      "  (24999, 415022)\t0.15795741326287652\n",
      "  (24999, 563810)\t0.15795741326287652\n",
      "  (24999, 322640)\t0.15795741326287652\n",
      "  (24999, 919279)\t0.15795741326287652\n",
      "  (24999, 1141016)\t0.15795741326287652\n",
      "  (24999, 473241)\t0.15795741326287652\n",
      "Testing Features  (0, 1780157)\t0.1499854136938126\n",
      "  (0, 1779450)\t0.05395490310050869\n",
      "  (0, 1776609)\t0.12547908680391487\n",
      "  (0, 1776348)\t0.06740479335714157\n",
      "  (0, 1769321)\t0.13197652489594064\n",
      "  (0, 1768503)\t0.09173551766465445\n",
      "  (0, 1721677)\t0.14415671646459366\n",
      "  (0, 1720458)\t0.03622851811426061\n",
      "  (0, 1693502)\t0.05604955936836909\n",
      "  (0, 1514058)\t0.13681342640505886\n",
      "  (0, 1513799)\t0.06533081345189293\n",
      "  (0, 1439027)\t0.05310518419655204\n",
      "  (0, 1414507)\t0.14002118996659385\n",
      "  (0, 1414405)\t0.13560243732795568\n",
      "  (0, 1401788)\t0.1499854136938126\n",
      "  (0, 1401583)\t0.06571417846843204\n",
      "  (0, 1391773)\t0.036596043947204365\n",
      "  (0, 1319673)\t0.10728470722394959\n",
      "  (0, 1319544)\t0.06715398810917607\n",
      "  (0, 1297007)\t0.14415671646459366\n",
      "  (0, 1296575)\t0.06217890749774713\n",
      "  (0, 1267449)\t0.14415671646459366\n",
      "  (0, 1267235)\t0.07790800887696729\n",
      "  (0, 1239400)\t0.08371284662160926\n",
      "  (0, 1229809)\t0.11204807744150319\n",
      "  :\t:\n",
      "  (24999, 162263)\t0.021471831641474322\n",
      "  (24999, 159290)\t0.05624557966705092\n",
      "  (24999, 147992)\t0.05463202262431973\n",
      "  (24999, 141594)\t0.02556832995058251\n",
      "  (24999, 136090)\t0.03497840917741073\n",
      "  (24999, 130528)\t0.03646685723499066\n",
      "  (24999, 129825)\t0.02215717658590282\n",
      "  (24999, 124550)\t0.032004691910640336\n",
      "  (24999, 123150)\t0.05851976058904544\n",
      "  (24999, 123084)\t0.03473092107669882\n",
      "  (24999, 119865)\t0.05338045054791431\n",
      "  (24999, 91996)\t0.023742218975037463\n",
      "  (24999, 91646)\t0.05463202262431973\n",
      "  (24999, 91613)\t0.04003400392888474\n",
      "  (24999, 90567)\t0.037900050817776004\n",
      "  (24999, 79273)\t0.031175654963993264\n",
      "  (24999, 52919)\t0.03882413331061624\n",
      "  (24999, 52700)\t0.029031413913970967\n",
      "  (24999, 48713)\t0.03331238383548767\n",
      "  (24999, 46513)\t0.06021370696497163\n",
      "  (24999, 13080)\t0.0465165140864366\n",
      "  (24999, 13078)\t0.030071577766948996\n",
      "  (24999, 82)\t0.04847010373759949\n",
      "  (24999, 81)\t0.03215561097831418\n",
      "  (24999, 0)\t0.03782949868901149\n",
      "[1 0 1 ... 0 0 1]\n",
      "Accuracy on the IMDB dataset: 88.66\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Transform each text into a vector of word counts\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\",\n",
    "                             preprocessor=clean_text,\n",
    "                             ngram_range=(1, 2))\n",
    "training_features = vectorizer.fit_transform(train_data[\"text\"]) \n",
    "print('Training Features' + str(training_features))\n",
    "test_features = vectorizer.transform(test_data[\"text\"])\n",
    "print('Testing Features' + str(test_features))\n",
    "# Training\n",
    "model = LinearSVC()\n",
    "model.fit(training_features, train_data[\"sentiment\"])\n",
    "y_pred = model.predict(test_features)\n",
    "print(y_pred)\n",
    "# Evaluation\n",
    "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
    "print(\"Accuracy on the IMDB dataset: {:.2f}\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we achieve an even higher accuracy score of 88.66% which is another 2% improvement over the last version of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
